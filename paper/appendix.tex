\appendix
The source code for re-running all the experiments detailed here and all data are available from \url{https://github.com/Corleno/RGPLVM}.

\section{Regularization Theorems} 
\label{sec: rt}
\begin{lemma}
	When $q(\bm z_m) = \mathcal{N}(\bm \nu_m, \epsilon I)$, as $\epsilon \rightarrow 0$, $\bm z_m \stackrel{p}{\rightarrow} \bm \nu_m$.
\end{lemma}

\begin{proof} Since
	\begin{eqnarray}
	\lim\limits_{\epsilon \rightarrow 0} p(|\bm z_m - \bm \nu_m| > \epsilon_0) & = & \lim\limits_{\epsilon \rightarrow 0} p(|\frac{\bm z_m - \bm \nu_m}{\epsilon}| > \frac{\epsilon_0}{\epsilon}) \nonumber \\
	& = & 2\lim\limits_{\epsilon \rightarrow 0} (1 - \Phi(\frac{\epsilon_0}{\epsilon}))^{Q} \nonumber \\
	& = & 0 \,, \nonumber
	\end{eqnarray}
	we conclude that $\bm z_m \stackrel{p}{\rightarrow} \bm \nu_m$.
\end{proof}

\begin{lemma}
	The variational lower bound in the empirical Bayesian model is derived as 
	\begin{eqnarray}
	\log p(Y) & \geq & E_{q(F, U, X, Z)}\log p(Y|F) - \mathrm{KL}(q(X)||p(X)) - \mathrm{KL}(q(U)||p(U)) - A + B + C \nonumber
	\end{eqnarray}
	where $A = \frac{M}{2}(\log |\hat{\Sigma}_{\bm \mu}| + \log|\hat{\bm\Sigma}_{\bm\nu}| + Q) + \frac{1}{2}\left(\sum_{m = 1}^M(\bm\nu_m - \hat{\bm \mu}_{\bm \mu})^T\hat{\Sigma}_{\bm\mu}^{-1}(\bm\nu_m - \hat{\bm \mu}_{\bm \mu})\right)$, $B = \frac{M}{2}(Q\log\epsilon-\log K)$ and $C = \frac{2\epsilon}{M\mathrm{tr}(\hat{\Sigma}_{\bm\mu}^{-1})}$.
\end{lemma}

\begin{proof}
	{\footnotesize
		\begin{eqnarray}
		\log p(\bm Y) & \geq &  E_{q(F, U, X, Z)}\log p(Y|F) - \mathrm{KL}(q(\bm X)|| p(\bm X)) - \mathrm{KL}(q(\bm U) ||p(\bm U )) -\mathrm{KL}(q(\bm Z)|| p(\bm Z)) \nonumber \\
		& = & E_{q(F, U, X, Z)}\log p(Y|F) - \mathrm{KL}(q(\bm X)|| p(\bm X)) - \mathrm{KL}(q(\bm U) ||p(\bm U )) - A \nonumber \\
		& & + \frac{M}{2}(Q\log\epsilon-\log |\hat{\bm\Sigma}_{\bm \nu}|) + C \nonumber \\
		& \geq & E_{q(F, U, X, Z)}\log p(Y|F) - \mathrm{KL}(q(X)||p(X)) - \mathrm{KL}(q(U)||p(U)) - A + B + C \nonumber
		\end{eqnarray}
	}
\end{proof}

\begin{lemma}
	We derive the regularization term as $M\mathrm{KL}(q_Z || q_X) = \frac{M}{2}(\log |\hat{\Sigma}_{\bm \mu}| + \log|\hat{\bm\Sigma}_{\bm z}| + Q) + \frac{1}{2}\left(\sum_{m = 1}^M(\bm z_m - \hat{\bm \mu}_{\bm \mu})^T\hat{\Sigma}_{\bm\mu}^{-1}(\bm z_m - \hat{\bm \mu}_{\bm \mu})\right)$.
\end{lemma}

\begin{proof}
	{\footnotesize
		\begin{eqnarray}
		\mathrm{KL}(q_Z || q_X) & = & \frac{1}{2}\left[\log\frac{|\hat{\Sigma}_{\bm\mu}|}{|\hat{\Sigma}_Z|} - Q + \mathrm{tr}(\hat{\Sigma}_{\bm\mu}^{-1}\hat{\Sigma}_Z)+(\hat{\bm \mu}_{\bm\mu} - \hat{\bm \mu}_Z)^T\hat{\Sigma}_{\bm\mu}^{-1}(\hat{\bm \mu}_{\bm\mu} - \hat{\bm \mu}_Z)\right] \nonumber \\
		& = & \frac{1}{2}\left[\log\frac{|\hat{\Sigma}_{\bm\mu}|}{|\hat{\Sigma}_Z|} - Q + \mathrm{tr}\left(\hat{\Sigma}_{\bm\mu}^{-1}((\hat{\bm \mu}_{\bm\mu} - \hat{\bm \mu}_Z)(\hat{\bm \mu}_{\bm\mu} - \hat{\bm \mu}_Z)^T + \hat{\Sigma}_Z)\right)\right] \nonumber \\
		& = & \frac{1}{2}\Bigg[\log\frac{|\hat{\Sigma}_{\bm\mu}|}{|\hat{\Sigma}_Z|} - Q + \frac{1}{M}\mathrm{tr}\bigg(\hat{\Sigma}_{\bm\mu}^{-1}(M(\hat{\bm \mu}_{\bm\mu} - \hat{\bm \mu}_Z)(\hat{\bm \mu}_{\bm\mu} - \hat{\bm \mu}_Z)^T + \sum_{m = 1}^{M}(\bm u_m - \hat{\bm \mu}_Z)(\bm u_m - \hat{\bm \mu}_Z)^T\bigg)\Bigg] \nonumber \\
		& = & \frac{1}{2}\Bigg[\log\frac{|\hat{\Sigma}_{\bm\mu}|}{|\hat{\Sigma}_Z|} - Q + \frac{1}{M}\mathrm{tr}\bigg(\hat{\Sigma}_{\bm\mu}^{-1}(M\hat{\bm \mu}_{\bm\mu}\hat{\bm \mu}_{\bm\mu}^T - M\hat{\bm \mu}_{\bm\mu}\hat{\bm \mu}_Z^T - M\hat{\bm \mu}_Z\hat{\bm \mu}_{\bm\mu}^T + M\hat{\bm \mu}_Z\hat{\bm \mu}_Z^T \nonumber \\ 
		& & + \sum_{m = 1}^{M}\bm u_m \bm u_m^T - (\sum_{m = 1}^{M}\bm z_m)\hat{\bm \mu}_Z^T - \hat{\bm \mu}_Z(\sum_{m = 1}^{M}\bm z_m)^T + M\hat{\bm \mu}_Z\hat{\bm \mu}_Z^T \bigg)\Bigg] \nonumber \\
		& = &  \frac{1}{2}\left[\log\frac{|\hat{\Sigma}_{\bm\mu}|}{|\hat{\Sigma}_Z|} - Q + \frac{1}{M}\mathrm{tr}\bigg(\hat{\Sigma}_{\bm\mu}^{-1}(\sum_{m = 1}^{M}\bm z_m\bm z_m^T - M\hat{\bm \mu}_{\bm\mu}\hat{\bm \mu}_Z^T - M\hat{\bm \mu}_Z\hat{\bm \mu}_{\bm\mu}^T + M\hat{\bm \mu}_X\hat{\bm \mu}_{\bm\mu}^T)\bigg)\right]\nonumber \\
		& = &  \frac{1}{2}\Bigg[\log\frac{|\hat{\Sigma}_{\bm\mu}|}{|\hat{\Sigma}_Z|} - Q + \frac{1}{M}\mathrm{tr}\bigg(\hat{\Sigma}_{\bm\mu}^{-1}(\sum_{m = 1}^{M}\bm z_m\bm z_m^T - \hat{\bm \mu}_{\bm\mu}(\sum_{m = 1}^{M}\bm z_m)^T - (\sum_{m = 1}^{M}\bm z_m)\hat{\bm \mu}_{\bm\mu}^T + M\hat{\bm \mu}_{\bm\mu}\hat{\bm \mu}_{\bm\mu}^T)\bigg)\Bigg]\nonumber \\
		& = & \frac{1}{2}\Bigg[\log\frac{|\hat{\Sigma}_{\bm\mu}|}{|\hat{\Sigma}_Z|} - Q + \frac{1}{M}\mathrm{tr}\bigg(\hat{\Sigma}_{\bm\mu}^{-1}(\sum_{m = 1}^{M}\bm z_m\bm z_m^T - \hat{\bm \mu}_{\bm\mu}(\sum_{m = 1}^{M}\bm u_m)^T - (\sum_{m = 1}^{M}\bm z_m)\hat{\bm \mu}_{\bm\mu}^T + M\hat{\bm \mu}_{\bm\mu}\hat{\bm \mu}_{\bm\mu}^T)\bigg)\Bigg]\nonumber \\
		& = & \frac{1}{2}\Bigg[\log\frac{|\hat{\Sigma}_{\bm\mu}|}{|\hat{\Sigma}_Z|} - Q + \frac{1}{M}\mathrm{tr}\bigg(\hat{\Sigma}_{\bm\mu}^{-1}(\sum_{m = 1}^{M}(\bm z_m - \hat{\bm \mu}_{\bm\mu})(\bm z_m - \hat{\bm \mu}_{\bm\mu})^T)\bigg)\Bigg]\nonumber \\
		& = & \frac{1}{M}\sum_{m = 1}^{M}\frac{1}{2}\left[\log\frac{|\hat{\Sigma}_{\bm\mu}|}{|\hat{\Sigma}_Z|} - Q + (\bm z_m - \hat{\bm \mu}_{\bm\mu})^T\hat{\Sigma}_{\bm\mu}^{-1}(\bm z_m - \hat{\bm \mu}_{\bm\mu})\right]\nonumber \,.
		\end{eqnarray}
	}
	
	Therefore, $ M\mathrm{KL}(q_Z || q_X) = \frac{1}{2}\sum_{m = 1}^{M}\left[\log\frac{|\hat{\Sigma}_{\bm\mu}|}{|\hat{\Sigma}_Z|} - Q + (\bm z_m - \hat{\bm \mu}_{\bm\mu})^T\hat{\Sigma}_{\bm\mu}^{-1}(\bm z_m - \hat{\bm \mu}_{\bm\mu})\right] =\frac{M}{2}(\log |\hat{\Sigma}_{\bm \mu}| + \log|\hat{\bm\Sigma}_{\bm z}| + Q) + \frac{1}{2}\left(\sum_{m = 1}^M(\bm z_m - \hat{\bm \mu}_{\bm \mu})^T\hat{\Sigma}_{\bm\mu}^{-1}(\bm z_m - \hat{\bm \mu}_{\bm \mu})\right)$. 
\end{proof}

\begin{theorem}
	As $\epsilon \rightarrow 0$, maximizing the variational lower bound in empirical Bayesian model is equivalent to maximizing the MELBO in the GPLVM with respect to $Z, q(X)$ and $q(U)$.
\end{theorem}

\begin{proof}
    In the empirical Bayesian model, variational parameters are $\bm \mu, \bm \Sigma, \bm m, \bm s, \bm \nu$. We denote all parameters as $\varTheta$. We have $\lim\limits_{\epsilon \rightarrow 0}C = 0$ and 
	\begin{eqnarray}
    \lim\limits_{\epsilon \rightarrow 0}E_{q(F, U, X, Z)}\log p(Y|F) & = & E_{q(F, U, X| Z = \bm \nu)}\log p(Y|F)
	\end{eqnarray}
	Then according to Lemma 2, we derive that
	\begin{eqnarray}
	& \arg\max\limits_{\varTheta}\lim\limits_{\epsilon \rightarrow 0} E_{q(F, U, X, Z)}\log p(Y|F) - \mathrm{KL}(q(X)||p(X)) - \mathrm{KL}(q(U)||p(U)) - A + B + C \nonumber \\
	= & 
	\arg\max\limits_{\varTheta}\lim\limits_{\epsilon\rightarrow 0} E_{q(F, U, X, Z)}\log p(Y|F) - \mathrm{KL}(q(X)||p(X)) - \mathrm{KL}(q(U)||p(U)) - A  \nonumber \\  
	= & \arg\max\limits_{\varTheta} E_{q(F, U, X| Z=\bm \nu)}\log p(Y|F) - \mathrm{KL}(q(X)|| p(X)) - \mathrm{KL}(q(U) ||p(U )) - A \nonumber 
	\end{eqnarray}
	When we replace $\bm \nu$ as $Z$, due to Lemma 3, this optimization is equivalent to maximize $\mathrm{ELBO} - M\mathrm{KL}(q_Z||q_X)$ which is the exactly MELBO. Finally due to Lemma 1, the $q(Z)$ in empirical Bayesian model will converges to the same optimized $Z$ in the GPLVM with regularization.
\end{proof}

\section{Stochastic Variational Inference Algorithm}
\label{sec: scia}
This section displays a general stochastic variational inference algorithm for large datasets. In this framework, we set the number of training epochs $N_{train}$ and evenly divide the whole dataset into $N_{batch}$ clusters. Each cluster includes the observations $\bm Y_i$ and their corresponding time stamp data $B_i$ and their corresponding hyper-parameters of embedding inputs, $\bm m_i$ for the mean and $\bm s_i$ for the standard deviation. In the context of the TCLGP, the model parameters include $\bm \theta, \bm Z, \bm \mu, \bm \Sigma$ and the model inputs include both observable data $\bm Y_i, B_i$ and latent hyper-parameters $\bm m_i, \bm s_i$. Suppose the MELBO(g) is rewritten as
\begin{eqnarray}
\mathrm{MELBO(g)} & = & \mathrm{ELBO(g)} - \lambda\mathrm{R} \nonumber \\
& = & gR_0 + R_1 - \lambda\mathrm{R}\,, \nonumber \\
R_0 & = & -\mathrm{KL}(q(\bm X) || p(\bm X | B)) - \mathrm{KL}(q(\bm U) || p(\bm U)) \,, \nonumber \\
R_1 & = & \int q(\bm X) q(\bm  U) p(\bm F | \bm X, \bm U) \log\left(\sum_{n = 1}^{N}\sum_{d = 1}^{D}\sum_{t = 1}^{T} p(\bm y_{ndt} | \bm f_{ndt})\right)d\bm X \bm U \bm F \,, \nonumber
\end{eqnarray}
where $R_0$ and $R_1$ are the regularization term and the reconstruction term 
in the ELBO separately and $\mathrm{R}$ is a regularization term related to inducing inputs, and $g\in[0,1])$ is the annealing factor referred in \cite{Bowman_2015}. The annealing increase factor is denoted as $\Delta g$. Then inference algorithm is displayed as follows: 

\begin{algorithm}[H]
	\SetAlgoLined
	Set $g = 0$\;
	\For{$i = 1$ \KwTo $N_{train}$}{
		\For{$j = 1$ \KwTo $N_{batch}$}{
			Assign both observable data $\bm Y_i, B_i$ and latent data $\bm m_i, \bm s_i$ to the TCLGP model\;
			Update model parameters $\bm \theta, \bm Z, \bm \mu, \bm \Sigma$ and latent hyper-parameters $\bm m_i, \bm s_i$ through maximizing the MELBO(g) using a stochastic gradient descend method.\;
			}
		$g = \min(g + \Delta g, 1)$\;
		}
	\caption{Stochastic variational inference algorithm for large datasets.}
\end{algorithm}
