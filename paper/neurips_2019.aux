\relax 
\providecommand\hyper@newdestlabel[2]{}
\providecommand\HyperFirstAtBeginDocument{\AtBeginDocument}
\HyperFirstAtBeginDocument{\ifx\hyper@anchor\@undefined
\global\let\oldcontentsline\contentsline
\gdef\contentsline#1#2#3#4{\oldcontentsline{#1}{#2}{#3}}
\global\let\oldnewlabel\newlabel
\gdef\newlabel#1#2{\newlabelxx{#1}#2}
\gdef\newlabelxx#1#2#3#4#5#6{\oldnewlabel{#1}{{#2}{#3}}}
\AtEndDocument{\ifx\hyper@anchor\@undefined
\let\contentsline\oldcontentsline
\let\newlabel\oldnewlabel
\fi}
\fi}
\global\let\hyper@last\relax 
\gdef\HyperFirstAtBeginDocument#1{#1}
\providecommand\HyField@AuxAddToFields[1]{}
\providecommand\HyField@AuxAddToCoFields[2]{}
\citation{Cressie_1993}
\citation{Banerjee_2008}
\citation{Rasmussen_2004}
\citation{Rasmussen_2005}
\citation{Seeger_2003}
\citation{Snelson_2006}
\citation{Finley_2009}
\citation{Snelson_2007}
\citation{Csato_2002}
\citation{Raj_2011}
\citation{Titsias_2009}
\citation{Hensman_2012,Hensman_2013}
\providecommand \oddpage@label [2]{}
\@writefile{toc}{\contentsline {section}{\numberline {1}Introduction}{1}{section.1}}
\citation{Lawrence_2003}
\citation{Lawrence_2003,Lawrence_2005}
\citation{Lawrence_2005}
\citation{Mardia_1979}
\citation{Scholkopf_1998}
\citation{Titsias_2010}
\citation{Titsias_2009}
\citation{Hensman_2013}
\citation{Lawrence_2007_HGP,Lawrence_2006,Urtasun_2007}
\citation{Hensman_2013}
\citation{Gal_2015}
\citation{Damianou_2016}
\citation{Titsias_2009}
\citation{Hensman_2013}
\citation{Raj_2011}
\citation{Titsias_2010,Hensman_2013}
\@writefile{lot}{\contentsline {table}{\numberline {1}{\ignorespaces Log likelihood ($\ell $) and root mean square error (RMSE) for model prediction on 1-D synthetic data with different setting of fixed inducing points.\relax }}{2}{table.caption.3}}
\newlabel{tab:SGP}{{1}{2}{Log likelihood ($\ell $) and root mean square error (RMSE) for model prediction on 1-D synthetic data with different setting of fixed inducing points.\relax }{table.caption.3}{}}
\@writefile{toc}{\contentsline {section}{\numberline {2}Sparse Gaussian Process}{2}{section.2}}
\newlabel{sec:SGP}{{2}{2}{Sparse Gaussian Process}{section.2}{}}
\citation{Titsias_2009,Titsias_2010}
\citation{Titsias_2010,Hensman_2013}
\citation{Titsias_2010}
\citation{Titsias_2009}
\citation{Hensman_2013}
\@writefile{lof}{\contentsline {figure}{\numberline {1}{\ignorespaces Stochastic variational Gaussian process on 1D synthetic data with different setting inducing inputs.\relax }}{3}{figure.caption.2}}
\providecommand*\caption@xref[2]{\@setref\relax\@undefined{#1}}
\newlabel{fig:SGP}{{1}{3}{Stochastic variational Gaussian process on 1D synthetic data with different setting inducing inputs.\relax }{figure.caption.2}{}}
\@writefile{toc}{\contentsline {section}{\numberline {3}Regularization for Latent Sparse Gaussian Process}{3}{section.3}}
\newlabel{sec:R}{{3}{3}{Regularization for Latent Sparse Gaussian Process}{section.3}{}}
\newlabel{GPLVM}{{1}{3}{Regularization for Latent Sparse Gaussian Process}{equation.3.1}{}}
\newlabel{GPLVM_VD}{{2}{3}{Regularization for Latent Sparse Gaussian Process}{equation.3.2}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.1}Regularization}{3}{subsection.3.1}}
\newlabel{sec:r}{{3.1}{3}{Regularization}{subsection.3.1}{}}
\newlabel{MELBO}{{4}{3}{Regularization}{equation.3.4}{}}
\citation{Gal_2015}
\citation{Lawrence_2007_HGP,Damianou_2016}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.2}Regularization Theory}{4}{subsection.3.2}}
\newlabel{sec:rt}{{3.2}{4}{Regularization Theory}{subsection.3.2}{}}
\citation{Gal_2015}
\@writefile{toc}{\contentsline {section}{\numberline {4}Temporal Categorical Latent Gaussian Process}{5}{section.4}}
\newlabel{sec:TCLGP}{{4}{5}{Temporal Categorical Latent Gaussian Process}{section.4}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.1}Model}{5}{subsection.4.1}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.2}Inference}{5}{subsection.4.2}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.3}Prediction}{5}{subsection.4.3}}
\@writefile{toc}{\contentsline {section}{\numberline {5}Experiments}{5}{section.5}}
\newlabel{sec:E}{{5}{5}{Experiments}{section.5}{}}
\citation{Joao_2014}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.1}Anuran}{6}{subsection.5.1}}
\@writefile{lof}{\contentsline {figure}{\numberline {2}{\ignorespaces Latent means of GPLVM for Anuran Call data set. Left panel refers to no regularization and the other three have regularization with different weights $\lambda = 20, 50, 100$. Different colors denote different genus types and crosses denote inducing inputs.\relax }}{6}{figure.caption.4}}
\newlabel{fig:ANURAN}{{2}{6}{Latent means of GPLVM for Anuran Call data set. Left panel refers to no regularization and the other three have regularization with different weights $\lambda = 20, 50, 100$. Different colors denote different genus types and crosses denote inducing inputs.\relax }{figure.caption.4}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.2}Synthetic Time Series}{6}{subsection.5.2}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.3}Stock Index}{6}{subsection.5.3}}
\@writefile{lof}{\contentsline {figure}{\numberline {3}{\ignorespaces Latent variables of TCLGP for synthetic data. Left panel refers to no regularization and the other three have regularization with different weights $\lambda = 20, 50, 100$. Different colors denote different observations and crosses denote inducing inputs.\relax }}{7}{figure.caption.5}}
\newlabel{fig:SIM}{{3}{7}{Latent variables of TCLGP for synthetic data. Left panel refers to no regularization and the other three have regularization with different weights $\lambda = 20, 50, 100$. Different colors denote different observations and crosses denote inducing inputs.\relax }{figure.caption.5}{}}
\@writefile{lot}{\contentsline {table}{\numberline {2}{\ignorespaces Evidence lower bound (ELBO) and modified evidence lower bound of GPLVMs with and without regularization for synthetic categorical time series.\relax }}{7}{table.caption.6}}
\newlabel{tab:SIM}{{2}{7}{Evidence lower bound (ELBO) and modified evidence lower bound of GPLVMs with and without regularization for synthetic categorical time series.\relax }{table.caption.6}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {5.3.1}Data Description}{7}{subsubsection.5.3.1}}
\@writefile{lot}{\contentsline {table}{\numberline {3}{\ignorespaces Percentiles of return rate for three stock indices from 1965 to 2012.\relax }}{7}{table.caption.7}}
\newlabel{quan}{{3}{7}{Percentiles of return rate for three stock indices from 1965 to 2012.\relax }{table.caption.7}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {5.3.2}Hyper-parameters Analysis}{7}{subsubsection.5.3.2}}
\bibstyle{apalike}
\bibdata{ref}
\bibcite{Banerjee_2008}{{1}{2008}{{Banerjee et~al.}}{{}}}
\bibcite{Scholkopf_1998}{{2}{1998}{{Bernhard et~al.}}{{}}}
\@writefile{lot}{\contentsline {table}{\numberline {4}{\ignorespaces Model fitting and prediction results of TCLGP with regularization under different settings of hyper-parameters. We evaluate model fitting by modified evidence lower bound (MELBO) and evaluate model prediction by training predictive accuracy (TRPA), training mean absolute difference (TRMAD), testing predictive accuracy(TRA) and testing mean absolute difference (TMAD).\relax }}{8}{table.caption.8}}
\newlabel{tab:STOCK}{{4}{8}{Model fitting and prediction results of TCLGP with regularization under different settings of hyper-parameters. We evaluate model fitting by modified evidence lower bound (MELBO) and evaluate model prediction by training predictive accuracy (TRPA), training mean absolute difference (TRMAD), testing predictive accuracy(TRA) and testing mean absolute difference (TMAD).\relax }{table.caption.8}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {5.3.3}Latent Processes Visualization}{8}{subsubsection.5.3.3}}
\@writefile{lof}{\contentsline {figure}{\numberline {4}{\ignorespaces Categorical plot (left) and predictive posterior latent process (middle) and estimated latent tracing (right) of stock indices in Year 2008 and Year 2009.\relax }}{8}{figure.caption.9}}
\newlabel{fig:stock}{{4}{8}{Categorical plot (left) and predictive posterior latent process (middle) and estimated latent tracing (right) of stock indices in Year 2008 and Year 2009.\relax }{figure.caption.9}{}}
\@writefile{toc}{\contentsline {section}{\numberline {6}Conclusion}{8}{section.6}}
\newlabel{sec:C}{{6}{8}{Conclusion}{section.6}{}}
\bibcite{Bowman_2015}{{3}{2015}{{{Bowman} et~al.}}{{}}}
\bibcite{Damianou_2016}{{4}{2016}{{Damianou et~al.}}{{}}}
\bibcite{Finley_2009}{{5}{2009}{{Finley et~al.}}{{}}}
\bibcite{Gal_2015}{{6}{2015}{{{Gal} et~al.}}{{}}}
\bibcite{Raj_2011}{{7}{2011}{{Guhaniyogi et~al.}}{{}}}
\bibcite{Cressie_1993}{{8}{1993}{{Haining}}{{}}}
\bibcite{Hensman_2013}{{9}{2013}{{{Hensman} et~al.}}{{}}}
\bibcite{Hensman_2012}{{10}{2012}{{{Hensman} et~al.}}{{}}}
\bibcite{Csato_2002}{{11}{2002}{{L. and Opper}}{{}}}
\bibcite{Lawrence_2005}{{12}{2005}{{Lawrence}}{{}}}
\bibcite{Lawrence_2003}{{13}{2003}{{Lawrence}}{{}}}
\bibcite{Lawrence_2007_HGP}{{14}{2007}{{Lawrence and Moore}}{{}}}
\bibcite{Lawrence_2006}{{15}{2006}{{Lawrence and Qui\~{n}onero Candela}}{{}}}
\bibcite{Mardia_1979}{{16}{1979}{{Mardia et~al.}}{{}}}
\bibcite{Joao_2014}{{17}{2014}{{Nicolau}}{{}}}
\bibcite{Rasmussen_2004}{{18}{2004}{{Rasmussen and Kuss}}{{}}}
\bibcite{Rasmussen_2005}{{19}{2005}{{Rasmussen and Williams}}{{}}}
\bibcite{Seeger_2003}{{20}{2003}{{Seeger}}{{}}}
\bibcite{Snelson_2006}{{21}{2006}{{Snelson and Ghahramani}}{{}}}
\bibcite{Snelson_2007}{{22}{2007}{{Snelson and Ghahramani}}{{}}}
\bibcite{Titsias_2009}{{23}{2009}{{Titsias}}{{}}}
\bibcite{Titsias_2010}{{24}{2010}{{Titsias and Lawrence}}{{}}}
\bibcite{Urtasun_2007}{{25}{2007}{{Urtasun and Darrell}}{{}}}
\@writefile{toc}{\contentsline {section}{\numberline {A}Regularization Theorems}{11}{appendix.A}}
\newlabel{sec: rt}{{A}{11}{Regularization Theorems}{appendix.A}{}}
\citation{Bowman_2015}
\@writefile{toc}{\contentsline {section}{\numberline {B}Stochastic Variational Inference Algorithm}{12}{appendix.B}}
\newlabel{sec: scia}{{B}{12}{Stochastic Variational Inference Algorithm}{appendix.B}{}}
\@writefile{loa}{\contentsline {algocf}{\numberline {1}{\ignorespaces Stochastic variational inference algorithm for large datasets.\relax }}{12}{algocf.1}}
